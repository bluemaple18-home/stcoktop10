"""
ETL ä¸»æµç¨‹
æ•´åˆæ‰€æœ‰æ¨¡çµ„çš„å®Œæ•´è³‡æ–™è™•ç†æµç¨‹
"""

import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import logging

from data_fetcher import DataFetcherOrchestrator
from indicators import TechnicalIndicators
from volume_indicators import VolumeIndicators
from fundamental_data import FundamentalData
from risk_filter import RiskFilter
from event_detector import EventDetector
try:
    from finmind_integrator import FinMindIntegrator
except ImportError:
    from app.finmind_integrator import FinMindIntegrator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ETLPipeline:
    """å®Œæ•´çš„ ETL æµç¨‹"""
    
    def __init__(self, data_dir: str = "data", artifacts_dir: str = "artifacts"):
        self.data_dir = Path(data_dir)
        self.raw_dir = self.data_dir / "raw"
        self.clean_dir = self.data_dir / "clean"
        self.artifacts_dir = Path(artifacts_dir)
        
        # å»ºç«‹ç›®éŒ„
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.clean_dir.mkdir(parents=True, exist_ok=True)
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        self.etl_stats = {}
    
    def run_full_pipeline(
        self,
        start_date: str = None,
        end_date: str = None,
        use_dummy_fundamental: bool = True,
        delay: float = 3.0
    ):
        """
        åŸ·è¡Œå®Œæ•´çš„ ETL æµç¨‹
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ 'YYYY-MM-DD'ï¼ˆé è¨­ç‚º 3 å¹´å‰ï¼‰
            end_date: çµæŸæ—¥æœŸ 'YYYY-MM-DD'ï¼ˆé è¨­ç‚ºä»Šæ—¥ï¼‰
            use_dummy_fundamental: æ˜¯å¦ä½¿ç”¨è™›æ“¬åŸºæœ¬é¢è³‡æ–™
            delay: API è«‹æ±‚å»¶é²ç§’æ•¸
        """
        logger.info("=" * 80)
        logger.info("é–‹å§‹åŸ·è¡Œ Agent A ç›¤å¾Œè³‡æ–™æ•´å‚™ ETL æµç¨‹")
        logger.info("=" * 80)
        
        # è¨­å®šæ—¥æœŸç¯„åœ
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')
        
        logger.info(f"è³‡æ–™å€é–“: {start_date} ~ {end_date}")
        
        # 1. è³‡æ–™æ“·å–
        logger.info("\n[éšæ®µ 1/7] è³‡æ–™æ“·å–")
        logger.info("-" * 80)
        orchestrator = DataFetcherOrchestrator(data_dir=str(self.raw_dir))
        
        # æ“·å–æ—¥è¡Œæƒ…
        df = orchestrator.fetch_historical_data(
            start_date=start_date,
            end_date=end_date,
            delay=delay
        )
        
        if df.empty:
            logger.error("è³‡æ–™æ“·å–å¤±æ•—ï¼Œä¸­æ­¢æµç¨‹")
            return
        
        self.etl_stats['data_fetching'] = {
            'total_records': len(df),
            'unique_stocks': df['stock_id'].nunique(),
            'date_range': f"{df['date'].min()} ~ {df['date'].max()}",
            'markets': df['market'].value_counts().to_dict() if 'market' in df.columns else {}
        }
        
        # æ“·å–è™•ç½®è‚¡æ¸…å–®
        suspended_list = orchestrator.fetch_suspended_stocks_list()
        
        # 1.1 æ•´åˆ FinMind ç±Œç¢¼é¢è³‡æ–™ (æ–°å¢)
        logger.info("\n[éšæ®µ 1.1] FinMind ç±Œç¢¼é¢è³‡æ–™æ•´åˆ")
        logger.info("-" * 80)
        # å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸æˆ–æª”æ¡ˆè®€å– token (æ­¤è™•æš«ç•™ null)
        finmind = FinMindIntegrator()
        df = finmind.integrate_chip_data(df)
        
        # 2. æŠ€è¡“æŒ‡æ¨™è¨ˆç®—
        logger.info("\n[éšæ®µ 2/7] æŠ€è¡“æŒ‡æ¨™è¨ˆç®—")
        logger.info("-" * 80)
        tech_ind = TechnicalIndicators(df)
        df = tech_ind.calculate_all_indicators()
        
        self.etl_stats['technical_indicators'] = {
            'missing_rate': tech_ind.get_missing_rate().to_dict()
        }
        
        # 3. é‡èƒ½æŒ‡æ¨™è¨ˆç®—
        logger.info("\n[éšæ®µ 3/7] é‡èƒ½æŒ‡æ¨™è¨ˆç®—")
        logger.info("-" * 80)
        vol_ind = VolumeIndicators(df)
        df = vol_ind.calculate_all_volume_indicators()
        
        self.etl_stats['volume_indicators'] = {
            'missing_rate': vol_ind.get_missing_rate().to_dict()
        }
        
        # 4. åŸºæœ¬é¢è³‡æ–™æ•´åˆ
        logger.info("\n[éšæ®µ 4/7] åŸºæœ¬é¢è³‡æ–™æ•´åˆ")
        logger.info("-" * 80)
        fundamental = FundamentalData(df)
        
        # æŠ“å–ç‡Ÿæ”¶è³‡æ–™ï¼ˆä½¿ç”¨æ‰¹æ¬¡æŠ“å–ï¼‰
        logger.info("æ‰¹æ¬¡æŠ“å–æœˆç‡Ÿæ”¶è³‡æ–™...")
        try:
            # è¨ˆç®—ç‡Ÿæ”¶æŠ“å–çš„æ—¥æœŸç¯„åœï¼ˆå–ä¸»è³‡æ–™çš„æ—¥æœŸç¯„åœï¼‰
            rev_start = df['date'].min().strftime('%Y-%m-%d')
            rev_end = df['date'].max().strftime('%Y-%m-%d')
            
            revenue_df = orchestrator.twse.fetch_revenue_batch(
                start_date=rev_start,
                end_date=rev_end,
                save_to_disk=True
            )
            
            if not revenue_df.empty:
                # åˆä½µç‡Ÿæ”¶è³‡æ–™
                df = fundamental.merge_revenue_data(revenue_df)
                logger.info(f"âœ… å·²æ•´åˆçœŸå¯¦ç‡Ÿæ”¶è³‡æ–™ï¼Œå…± {len(revenue_df)} ç­†")
                
                # çµ±è¨ˆç‡Ÿæ”¶è³‡æ–™è¦†è“‹ç‡
                revenue_coverage = (df['revenue_yoy'].notna().sum() / len(df)) * 100
                self.etl_stats['revenue_data'] = {
                    'total_records': len(revenue_df),
                    'coverage_rate': f"{revenue_coverage:.2f}%",
                    'yoy_mean': df['revenue_yoy'].mean(),
                    'mom_mean': df['revenue_mom'].mean()
                }
            else:
                logger.warning("ç‡Ÿæ”¶è³‡æ–™æŠ“å–å¤±æ•—ï¼Œä½¿ç”¨è™›æ“¬è³‡æ–™")
                df = fundamental.create_dummy_fundamental_data()
                self.etl_stats['revenue_data'] = {'status': 'dummy_data_used'}
                
        except Exception as e:
            logger.error(f"ç‡Ÿæ”¶è³‡æ–™è™•ç†å¤±æ•—: {e}")
            logger.warning("æ”¹ç”¨è™›æ“¬åŸºæœ¬é¢è³‡æ–™")
            df = fundamental.create_dummy_fundamental_data()
            self.etl_stats['revenue_data'] = {'status': f'error: {str(e)}'}
            
        # 5. å„²å­˜å®Œæ•´ç‰¹å¾µè³‡æ–™
        logger.info("\n[éšæ®µ 5/7] å„²å­˜å®Œæ•´ç‰¹å¾µè³‡æ–™")
        logger.info("-" * 80)
        features_path = self.clean_dir / "features.parquet"
        df.to_parquet(features_path, index=False)
        logger.info(f"features.parquet å·²å„²å­˜: {len(df)} ç­†, {df['stock_id'].nunique()} æª”è‚¡ç¥¨")
        
        self.etl_stats['features_file'] = {
            'path': str(features_path),
            'size_mb': features_path.stat().st_size / 1024 / 1024,
            'records': len(df),
            'stocks': df['stock_id'].nunique()
        }

        # 6. æŠ€è¡“äº‹ä»¶åµæ¸¬ (æ–°å¢)
        logger.info("\n[éšæ®µ 6/7] æŠ€è¡“äº‹ä»¶åµæ¸¬")
        logger.info("-" * 80)
        event_detector = EventDetector(df)
        events_df = event_detector.detect_all_events()
        
        events_path = self.clean_dir / "events.parquet"
        events_df.to_parquet(events_path, index=False)
        logger.info(f"events.parquet å·²å„²å­˜: {len(events_df)} ç­†, {len(events_df.columns)-2} å€‹äº‹ä»¶")
        
        self.etl_stats['events_file'] = {
            'path': str(events_path),
            'size_mb': events_path.stat().st_size / 1024 / 1024,
            'records': len(events_df),
            'events_count': len(events_df.columns) - 2
        }
        
        # 7. é¢¨éšªéæ¿¾
        logger.info("\n[éšæ®µ 7/7] é¢¨éšªéæ¿¾")
        logger.info("-" * 80)
        risk_filter = RiskFilter(df)
        universe = risk_filter.apply_all_filters(
            suspended_list=suspended_list,
            min_listing_days=60,
            min_avg_value=10_000_000,
            min_price=10.0
        )
        
        # å„²å­˜è‚¡ç¥¨æ± 
        universe_path = self.clean_dir / "universe.parquet"
        universe.to_parquet(universe_path, index=False)
        logger.info(f"universe.parquet å·²å„²å­˜: {len(universe)} ç­†, {universe['stock_id'].nunique()} æª”è‚¡ç¥¨")
        
        self.etl_stats['universe_file'] = {
            'path': str(universe_path),
            'size_mb': universe_path.stat().st_size / 1024 / 1024,
            'records': len(universe),
            'stocks': universe['stock_id'].nunique()
        }
        
        self.etl_stats['risk_filtering'] = risk_filter.get_filter_report().to_dict('records')
        
        # 8. ç”¢ç”Ÿå ±å‘Šèˆ‡è¦–è¦ºåŒ–
        logger.info("\n[éšæ®µ 8/8] ç”¢ç”Ÿå ±å‘Šèˆ‡è¦–è¦ºåŒ–")
        logger.info("-" * 80)
        
        # ç”¢ç”Ÿ ETL å ±å‘Š
        self.generate_etl_report(df, universe, events_df, orchestrator, tech_ind, vol_ind)
        
        # ç”¢ç”Ÿè¦–è¦ºåŒ–
        from visualization import generate_signals_preview
        preview_path = self.artifacts_dir / "signals_preview.png"
        generate_signals_preview(universe, output_path=str(preview_path), num_samples=5)
        
        logger.info("\n" + "=" * 80)
        logger.info("ETL æµç¨‹åŸ·è¡Œå®Œæˆï¼")
        logger.info("=" * 80)
        logger.info(f"\nç”¢å‡ºæª”æ¡ˆ:")
        logger.info(f"  - {features_path}")
        logger.info(f"  - {universe_path}")
        logger.info(f"  - {self.artifacts_dir / 'etl_report.md'}")
        logger.info(f"  - {preview_path}")
    
    def generate_etl_report(
        self,
        features_df: pd.DataFrame,
        universe_df: pd.DataFrame,
        events_df: pd.DataFrame,
        orchestrator: DataFetcherOrchestrator,
        tech_ind: TechnicalIndicators,
        vol_ind: VolumeIndicators
    ):
        """ç”¢ç”Ÿ ETL å ±å‘Š"""
        
        report_path = self.artifacts_dir / "etl_report.md"
        
        # è¨ˆç®—æ•´é«”ç¼ºå€¼ç‡
        all_missing = features_df.isnull().sum() / len(features_df) * 100
        main_indicators_missing = all_missing[all_missing.index.str.contains('ma|rsi|macd|obv|volume', case=False, na=False)]
        
        # è³‡æ–™ä¾†æºçµ±è¨ˆ
        data_quality = orchestrator.get_data_quality_report()
        source_stats = data_quality['source'].value_counts().to_dict() if not data_quality.empty else {}
        
        # ç”¢ç”Ÿå ±å‘Š
        report = f"""# Agent A ç›¤å¾Œè³‡æ–™æ•´å‚™ ETL å ±å‘Š

**åŸ·è¡Œæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š è³‡æ–™æ‘˜è¦

### å®Œæ•´ç‰¹å¾µè³‡æ–™ (features.parquet)

- **è³‡æ–™ç­†æ•¸**: {len(features_df):,} ç­†
- **è‚¡ç¥¨æ•¸é‡**: {features_df['stock_id'].nunique():,} æª”
- **æ¶µè“‹æ—¥æœŸ**: {features_df['date'].min().strftime('%Y-%m-%d')} ~ {features_df['date'].max().strftime('%Y-%m-%d')}
- **æª”æ¡ˆå¤§å°**: {self.etl_stats['features_file']['size_mb']:.2f} MB

### æŠ€è¡“äº‹ä»¶è³‡æ–™ (events.parquet)

- **è³‡æ–™ç­†æ•¸**: {len(events_df):,} ç­†
- **äº‹ä»¶æ•¸é‡**: {self.etl_stats['events_file']['events_count']} ç¨®
- **æª”æ¡ˆå¤§å°**: {self.etl_stats['events_file']['size_mb']:.2f} MB

### è‚¡ç¥¨æ±  (universe.parquet)

- **è³‡æ–™ç­†æ•¸**: {len(universe_df):,} ç­†
- **è‚¡ç¥¨æ•¸é‡**: {universe_df['stock_id'].nunique():,} æª”
- **æª”æ¡ˆå¤§å°**: {self.etl_stats['universe_file']['size_mb']:.2f} MB

> [!{"IMPORTANT" if universe_df['stock_id'].nunique() >= 500 else "WARNING"}]
> ä»Šæ—¥ universe è‚¡ç¥¨æ•¸: **{universe_df['stock_id'].nunique()} æª”**
> {"âœ… é”åˆ°é©—æ”¶æ¨™æº– (â‰¥ 500)" if universe_df['stock_id'].nunique() >= 500 else "âš ï¸ æœªé”é©—æ”¶æ¨™æº– (< 500)"}

---

## ğŸ“ˆ è³‡æ–™ä¾†æºçµ±è¨ˆ

| ä¾†æº | æˆåŠŸç­†æ•¸ | å¤±æ•—ç­†æ•¸ |
|------|---------|---------|
"""
        
        if not data_quality.empty:
            success_stats = data_quality[data_quality['status'] == 'success'].groupby('source').size()
            failed_stats = data_quality[data_quality['status'] != 'success'].groupby('source').size()
            
            all_sources = set(success_stats.index.tolist() + failed_stats.index.tolist())
            for source in sorted(all_sources):
                success = success_stats.get(source, 0)
                failed = failed_stats.get(source, 0)
                report += f"| {source} | {success:,} | {failed:,} |\n"
        else:
            report += "| N/A | N/A | N/A |\n"
        
        report += f"""
---

## ğŸ” ç¼ºå€¼ç‡åˆ†æ

### ä¸»è¦æŠ€è¡“æŒ‡æ¨™ç¼ºå€¼ç‡

| æŒ‡æ¨™ | ç¼ºå€¼ç‡ (%) |
|------|-----------|
"""
        
        for indicator, missing_pct in main_indicators_missing.head(10).items():
            status = "âœ…" if missing_pct < 1.0 else "âš ï¸"
            report += f"| {indicator} | {missing_pct:.2f}% {status} |\n"
        
        max_missing = main_indicators_missing.max()
        report += f"""
> [!{"IMPORTANT" if max_missing < 1.0 else "WARNING"}]
> ä¸»è¦æŒ‡æ¨™æœ€é«˜ç¼ºå€¼ç‡: **{max_missing:.2f}%**
> {"âœ… é”åˆ°é©—æ”¶æ¨™æº– (< 1%)" if max_missing < 1.0 else "âš ï¸ æœªé”é©—æ”¶æ¨™æº– (â‰¥ 1%)"}

---

## ğŸ›¡ï¸ é¢¨éšªéæ¿¾çµ±è¨ˆ

| éæ¿¾éšæ®µ | å‰©é¤˜è‚¡ç¥¨æ•¸ | ç§»é™¤è‚¡ç¥¨æ•¸ |
|---------|-----------|-----------|
"""
        
        for stage_info in self.etl_stats.get('risk_filtering', []):
            stage = stage_info.get('stage', 'N/A')
            remaining = stage_info.get('remaining_stocks', stage_info.get('unique_stocks', 'N/A'))
            removed = stage_info.get('removed_stocks', 0)
            
            if stage == 'initial':
                report += f"| åˆå§‹è³‡æ–™ | {remaining:,} | - |\n"
            else:
                report += f"| {stage} | {remaining:,} | {removed:,} |\n"
        
        report += f"""
---

## ğŸ“‹ æ¬„ä½æ¸…å–®

### åŸºæœ¬æ¬„ä½
- `date`: æ—¥æœŸ
- `stock_id`: è‚¡ç¥¨ä»£ç¢¼
- `stock_name`: è‚¡ç¥¨åç¨±
- `open`, `high`, `low`, `close`: é–‹é«˜ä½æ”¶
- `volume`: æˆäº¤é‡

### æŠ€è¡“æŒ‡æ¨™
- MA: `ma5`, `ma10`, `ma20`, `ma60`
- EMA: `ema12`, `ema26`
- MACD: `macd`, `macd_signal`, `macd_hist`
- RSI: `rsi`
- KD: `k`, `d`
- å¸ƒæ—é€šé“: `bb_upper`, `bb_middle`, `bb_lower`, `bb_width`
- å‰é«˜çªç ´: `breakout_flag`

### é‡èƒ½æŒ‡æ¨™
- å¹³å‡é‡: `avg_volume_5d`, `avg_volume_10d`, `avg_volume_20d`
- é‡æ¯”: `volume_ratio_5d`, `volume_ratio_10d`, `volume_ratio_20d`
- OBV: `obv`
- æ—¥å‡æˆäº¤å€¼: `avg_value_20d`

### åŸºæœ¬é¢æŒ‡æ¨™
- ç‡Ÿæ”¶: `revenue_yoy`, `revenue_mom`
- ç²åˆ©: `eps_4q`, `roe`, `gross_margin`
- æ®–åˆ©ç‡: `dividend_yield`

---

## âš ï¸ æ³¨æ„äº‹é …

> [!NOTE]
> åŸºæœ¬é¢è³‡æ–™ç›®å‰ä½¿ç”¨è™›æ“¬è³‡æ–™ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰ï¼Œå¯¦éš›æ‡‰ç”¨æ™‚éœ€æ•´åˆçœŸå¯¦è³‡æ–™ä¾†æºã€‚

> [!TIP]
> å»ºè­°æ¯æ—¥åŸ·è¡Œæ­¤ ETL æµç¨‹ï¼Œç¢ºä¿è³‡æ–™ä¿æŒæœ€æ–°ç‹€æ…‹ã€‚

---

**å ±å‘Šç”¢ç”Ÿæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # å¯«å…¥å ±å‘Š
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ETL å ±å‘Šå·²ç”¢ç”Ÿ: {report_path}")
    
    def validate(self):
        """é©—è­‰ç”¢å‡ºæ˜¯å¦ç¬¦åˆé©—æ”¶æ¨™æº–"""
        logger.info("åŸ·è¡Œé©—æ”¶æ¸¬è©¦...")
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        features_path = self.clean_dir / "features.parquet"
        universe_path = self.clean_dir / "universe.parquet"
        events_path = self.clean_dir / "events.parquet"
        
        if not features_path.exists() or not universe_path.exists() or not events_path.exists():
            logger.error("âŒ å¿…è¦æª”æ¡ˆä¸å­˜åœ¨")
            return False
        
        # è¼‰å…¥æª”æ¡ˆ
        universe = pd.read_parquet(universe_path)
        features = pd.read_parquet(features_path)
        events = pd.read_parquet(events_path)
        
        # æª¢æŸ¥ events èˆ‡ features ç”¨é‡æ˜¯å¦ä¸€è‡´ (ç°¡å–®æª¢æŸ¥è¡Œæ•¸)
        if len(events) != len(features):
             logger.warning(f"âš ï¸ events ({len(events)}) èˆ‡ features ({len(features)}) ç­†æ•¸ä¸ä¸€è‡´")
        else:
             logger.info("âœ… events èˆ‡ features ç­†æ•¸ä¸€è‡´")
             
        # æª¢æŸ¥äº‹ä»¶æ¬„ä½
        expected_events = [
            'break_20d_high', 'ma5_cross_ma20_up', 'close_above_bb_mid', 'macd_bullish_cross', 
            'rsi_rebound_from_40', 'gap_up_close_strong', 'volume_spike', 
            'lose_20d_low', 'ma5_cross_ma20_down', 'close_below_bb_mid', 'macd_bearish_cross', 
            'rsi_break_below_50', 'long_upper_shadow'
        ]
        
        missing_events = [e for e in expected_events if e not in events.columns]
        if missing_events:
            logger.warning(f"âš ï¸ events ç¼ºå°‘æ¬„ä½: {missing_events}")
        else:
            logger.info("âœ… events åŒ…å«æ‰€æœ‰é æœŸäº‹ä»¶æ¬„ä½")
        
        # æª¢æŸ¥è‚¡ç¥¨æ•¸
        stock_count = universe['stock_id'].nunique()
        if stock_count < 500:
            logger.warning(f"âš ï¸ universe è‚¡ç¥¨æ•¸ ({stock_count}) < 500")
        else:
            logger.info(f"âœ… universe è‚¡ç¥¨æ•¸ ({stock_count}) â‰¥ 500")
        
        # æª¢æŸ¥ç¼ºå€¼ç‡
        main_indicators = universe.filter(regex='ma|rsi|macd|obv|volume', axis=1)
        missing_rate = (main_indicators.isnull().sum() / len(universe) * 100).max()
        
        if missing_rate >= 1.0:
            logger.warning(f"âš ï¸ ä¸»è¦æŒ‡æ¨™ç¼ºå€¼ç‡ ({missing_rate:.2f}%) â‰¥ 1%")
        else:
            logger.info(f"âœ… ä¸»è¦æŒ‡æ¨™ç¼ºå€¼ç‡ ({missing_rate:.2f}%) < 1%")
        
        logger.info("é©—æ”¶æ¸¬è©¦å®Œæˆ")
        
        return stock_count >= 500 and missing_rate < 1.0 and len(missing_events) == 0


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Agent A ç›¤å¾Œè³‡æ–™æ•´å‚™ ETL æµç¨‹')
    parser.add_argument('--start-date', type=str, help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)', default=None)
    parser.add_argument('--end-date', type=str, help='çµæŸæ—¥æœŸ (YYYY-MM-DD)', default=None)
    parser.add_argument('--delay', type=float, help='API è«‹æ±‚å»¶é²ç§’æ•¸', default=3.0)
    parser.add_argument('--validate', action='store_true', help='åƒ…åŸ·è¡Œé©—æ”¶æ¸¬è©¦')
    
    args = parser.parse_args()
    
    pipeline = ETLPipeline()
    
    if args.validate:
        # åƒ…é©—è­‰
        pipeline.validate()
    else:
        # åŸ·è¡Œå®Œæ•´æµç¨‹
        pipeline.run_full_pipeline(
            start_date=args.start_date,
            end_date=args.end_date,
            delay=args.delay
        )
        
        # åŸ·è¡Œé©—æ”¶
        pipeline.validate()
