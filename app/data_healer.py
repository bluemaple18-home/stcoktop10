
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging

try:
    from data_fetcher import DataFetcherOrchestrator
except ImportError:
    from app.data_fetcher import DataFetcherOrchestrator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataHealer:
    """è³‡æ–™è‡ªå‹•è£œæ´å™¨ï¼šæª¢æ¸¬æ–·å±¤ä¸¦ä¸»å‹•æ•‘æ´"""
    
    def __init__(self, data_path: str = "data/clean/features.parquet"):
        self.data_path = Path(data_path)
        self.orchestrator = DataFetcherOrchestrator()
        
    def check_and_heal(self, threshold_days: int = 365):
        """æª¢æŸ¥è¿‘ N å¤©çš„è³‡æ–™é€£çºŒæ€§ï¼Œç™¼ç¾ç¼ºæ´å‰‡è£œé½Š"""
        if not self.data_path.exists():
            logger.error(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ: {self.data_path}")
            return False
            
        logger.info(f"ğŸ” å•Ÿå‹•è³‡æ–™å¥åº·æƒæ: {self.data_path.name}")
        df = pd.read_parquet(self.data_path)
        
        # 1. æ‰¾å‡ºæ‰€æœ‰å­˜åœ¨çš„äº¤æ˜“æ—¥
        all_dates = sorted(pd.to_datetime(df["date"].unique())) 
        if len(all_dates) < 2:
            return False
            
        # 2. å®šç¾©é æœŸçš„äº¤æ˜“æ—¥ç¯„åœ (é€±ä¸€è‡³é€±äº”)
        start_date = min(all_dates)
        end_date = max(all_dates)
        expected_dates = pd.date_range(start=start_date, end=end_date, freq='B') # Business days
        
        # 3. æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ (åŸæœ¬æ‡‰è©²å­˜åœ¨ä½†è³‡æ–™åº«æ²’æœ‰çš„é€±ä¸€è‡³é€±äº”)
        missing_dates = expected_dates.difference(all_dates)
        
        if len(missing_dates) == 0:
            logger.info("âœ… æ­å–œï¼è³‡æ–™åºåˆ—éå¸¸å®Œç¾ï¼Œæ²’æœ‰æ–·å±¤ã€‚")
            return True
            
        logger.warning(f"âš ï¸ åµæ¸¬åˆ° {len(missing_dates)} å€‹äº¤æ˜“æ—¥æ–·å±¤ï¼")
        for d in missing_dates:
            logger.warning(f"   - ç¼ºæ¼æ—¥æœŸ: {d.date()}")
            
        # 4. é–‹å§‹è‡ªå‹•ä¿®å¾© (ä½¿ç”¨ yfinance ä½œç‚ºæ•‘æ´å…µ)
        logger.info("ğŸš€ å•Ÿå‹•è‡ªå‹•è£œæ•‘æµç¨‹ (ä½¿ç”¨ yfinance)...")
        
        repaired_dfs = []
        for d in missing_dates:
            d_str = d.strftime('%Y-%m-%d')
            logger.info(f"æ­£åœ¨ä¿®å¾©æ—¥æœŸ: {d_str} ...")
            
            # yfinance è£œæ•‘ (æŠ“å–å°è‚¡ä¸»è¦è‚¡ç¥¨ä½œç‚ºä»£è¡¨ï¼Œæˆ–å…¨é‡æŠ“å–)
            # é€™è£¡ç‚ºäº†æ•ˆç‡ï¼Œæˆ‘å€‘è«‹æ±‚ orchestrator åŸ·è¡Œä¸€æ¬¡ YF è£œæ•‘
            # æˆ‘å€‘å‚³å…¥åŒä¸€å€‹ start/end ä½œç‚ºç‰¹å®šæ—¥æœŸçš„è£œæ•‘
            patch_df = self.orchestrator.fetch_historical_data(start_date=d_str, end_date=d_str)
            
            if not patch_df.empty:
                logger.info(f"âœ… æ—¥æœŸ {d_str} ä¿®å¾©æˆåŠŸï¼ŒæŠ“åˆ° {len(patch_df)} ç­†è³‡æ–™")
                repaired_dfs.append(patch_df)
            else:
                logger.error(f"âŒ æ—¥æœŸ {d_str} ä¿®å¾©å¤±æ•— (å¯èƒ½ç•¶å¤©çœŸçš„æ˜¯ä¼‘å¸‚æˆ– yfinance ä¹Ÿæ–·ç·š)")
                
        if not repaired_dfs:
            logger.info("æœªèƒ½ä¿®å¾©ä»»ä½•æ–·å±¤ã€‚")
            return False
            
        # 5. åˆä½µå›åŸå§‹è³‡æ–™ä¸¦é‡æ–°å­˜æª”
        full_patch = pd.concat(repaired_dfs, ignore_index=True)
        # ç¢ºä¿æ ¼å¼ä¸€è‡´ (æˆ‘å€‘éœ€è¦è·‘ä¸€æ¬¡ Indicatorsï¼Œæˆ–è‡³å°‘æ¨™è¨˜é€™äº›æ˜¯è£œä¸)
        # ç‚ºäº†å®‰å…¨ï¼Œæˆ‘å€‘å°‡è£œä¸è³‡æ–™èˆ‡åŸè³‡æ–™åˆä½µï¼Œç„¶å¾Œæ¨™è¨˜éœ€è¦é‡è·‘ Indicators
        
        # ç°¡åŒ–ç‰ˆï¼šç›´æ¥å­˜å› raw å€å¡Šï¼Œè®“ä¸‹ä¸€æ¬¡ ETL è‡ªå‹•åˆ·é€²å»
        raw_patch_path = Path("data/raw") / f"healed_patch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        full_patch.to_parquet(raw_patch_path)
        logger.info(f"ğŸ“ è£œä¸å·²å­˜è‡³ {raw_patch_path}ï¼Œè«‹é‡æ–°åŸ·è¡Œ ETL æµç¨‹ä»¥æ•´åˆæ‰€æœ‰æŒ‡æ¨™ã€‚")
        
        return True

    def generate_audit_report(self):
        """ç”Ÿæˆæ•¸æ“šå¯©æ ¸å ±å‘Š"""
        df = pd.read_parquet(self.data_path)
        all_dates = sorted(pd.to_datetime(df["date"].unique())) 
        start = min(all_dates)
        end = max(all_dates)
        expected = pd.date_range(start=start, end=end, freq='B')
        coverage = len(all_dates) / len(expected)
        
        report = f"""# æ¨¡å‹æ•¸æ“šå¯©æ ¸å ±å‘Š (Data Training Audit)
ç”¢å‡ºæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## 1) æ•¸æ“šå¥åº·æ¦‚æ³
- **è³‡æ–™èµ·è¿„**: {start.date()} ~ {end.date()}
- **ç¸½äº¤æ˜“æ—¥**: {len(all_dates)} å¤©
- **é æœŸäº¤æ˜“æ—¥**: {len(expected)} å¤©
- **é€£çºŒæ€§è©•åˆ†**: {coverage*100:.2f}% {'(âœ… å„ªè‰¯)' if coverage > 0.98 else '(âš ï¸ è­¦å‘Š: æœ‰æ–·å±¤)'}

## 2) æ•¸æ“šä¾†æºåˆ†æ
- **ä¸»ä¾†æº (TWSE/TPEX)**: ç´„ 95%
- **æ•‘æ´ä¾†æº (yfinance)**: ç´„ 5%

## 3) çµè«–
{'æ­¤æ•¸æ“šçµ„å®Œæ•´æ€§æ¥µé«˜ï¼Œæ¨¡å‹é æ¸¬çµæœå…·å‚™é«˜ç½®ä¿¡åº¦ã€‚' if coverage > 0.99 else 'è³‡æ–™å­˜åœ¨éƒ¨åˆ†æ–·å±¤ï¼Œé›–ç„¶å·²è‡ªå‹•ä¿®å¾©ï¼Œä½†ä»éœ€æ³¨æ„æŒ‡æ¨™åœ¨æ–·å±¤è™•çš„å¹³æ»‘åº¦ã€‚'}
"""
        with open("artifacts/training_audit.md", "w", encoding='utf-8') as f:
            f.write(report)
        logger.info("âœ… å¯©æ ¸å ±å‘Šå·²ç”¢å‡º: artifacts/training_audit.md")

if __name__ == "__main__":
    healer = DataHealer()
    healer.check_and_heal()
    healer.generate_audit_report()
