
import pandas as pd
import numpy as np
import lightgbm as lgb
from pathlib import Path
from datetime import datetime, timedelta
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit
import optuna
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.isotonic import IsotonicRegression
import shap
import mlflow
import mlflow.lightgbm
import webbrowser
import os

# å¼•å…¥æ–°çš„æ¨™ç±¤ç”Ÿæˆå™¨
try:
    from labels import LabelGenerator
except ImportError:
    # è‹¥ç›¸å°è·¯å¾‘åŒ¯å…¥å¤±æ•—ï¼Œå˜—è©¦å¾ app åŒ¯å…¥
    from app.labels import LabelGenerator


class LightGBMTrainer:
    """LightGBM åˆ†é¡æ¨¡å‹è¨“ç·´å™¨ (Advanced ç‰ˆ: Calibration + SHAP)"""
    
    def __init__(self, data_dir: str = "data/clean", model_dir: str = "models", 
                 artifact_dir: str = "artifacts", horizon: int = 10, threshold: float = 0.03):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        Args:
            horizon: æŒæœ‰å¤©æ•¸ (é è¨­ 10 å¤©)
            threshold: ç²åˆ©é–€æª» (é è¨­ 5%)
        """
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.artifact_dir = Path(artifact_dir)
        self.horizon = horizon
        self.threshold = threshold
        self.model = None
        self.calibrator = None  # æ©Ÿç‡æ ¡æº–å™¨
        self.best_params = None
        
        # å»ºç«‹å¿…è¦ç›®éŒ„
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.artifact_dir.mkdir(parents=True, exist_ok=True)
    
    def load_features(self, file_path: str = None) -> pd.DataFrame:
        """è¼‰å…¥ç‰¹å¾µè³‡æ–™"""
        if file_path is None:
            # å„ªå…ˆæœå°‹æ­£å¼è·¯å¾‘
            file_path = self.data_dir / "features.parquet"
            if not file_path.exists():
                file_path = Path("data/test/features_test.parquet")
        else:
            file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"ç‰¹å¾µæª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        
        df = pd.read_parquet(file_path)
        
        # è¨˜æ†¶é«”å„ªåŒ–
        float_cols = df.select_dtypes(include=['float64']).columns
        if len(float_cols) > 0:
            df[float_cols] = df[float_cols].astype('float32')
            
        print(f"âœ“ è¼‰å…¥ç‰¹å¾µè³‡æ–™: {len(df)} ç­†, {len(df.columns)} æ¬„ä½")
        return df
    
    def generate_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨ LabelGenerator ç”Ÿæˆæ¨™ç±¤"""
        generator = LabelGenerator(horizon=self.horizon, threshold=self.threshold)
        # éœ€ç¢ºä¿ df æœ‰ open/close æ¬„ä½ï¼ŒETL ç”¢å‡ºçš„ features.parquet æ‡‰è©²æœ‰
        if 'open' not in df.columns:
            print("âš  è­¦å‘Š: æ‰¾ä¸åˆ° 'open' æ¬„ä½ï¼Œæ¨™ç±¤ç”Ÿæˆå¯èƒ½å¤±æ•—")
        
        df_labeled = generator.generate_labels(df)
        return df_labeled
    
    def prepare_train_data(self, df: pd.DataFrame, exclude_cols: list = None):
        """æº–å‚™è¨“ç·´è³‡æ–™"""
        if exclude_cols is None:
            exclude_cols = [
                'symbol', 'stock_id', 'date', 'target', 'stock_name', 
                'entry_price', 'exit_price', 'return_5d', 'future_close',
                'return_long', 'future_return',
                'high_roll_20', 'low_roll_5', 'future_max_20d' # ä¿®æ­£ leakage
            ]
        
        y = df['target'] # 0 or 1
        
        # 1. å…ˆæ’é™¤æ˜ç¢ºæŒ‡å®šçš„æ¬„ä½
        potential_features = [col for col in df.columns if col not in exclude_cols]
        X_raw = df[potential_features]
        
        # 2. å¼·åˆ¶åƒ…ä¿ç•™æ•¸å€¼å‹åˆ¥ (int, float, bool)
        # LightGBM ä¸æ¥å— object / string
        X = X_raw.select_dtypes(include=[np.number, bool])
        
        # è¨˜éŒ„è¢«æ’é™¤çš„æ¬„ä½ (Debugç”¨)
        dropped = set(X_raw.columns) - set(X.columns)
        if dropped:
            print(f"âš  è‡ªå‹•æ’é™¤éæ•¸å€¼æ¬„ä½: {dropped}")
            
        feature_cols = X.columns.tolist()
        
        print(f"âœ“ æº–å‚™è¨“ç·´è³‡æ–™: {len(X)} ç­†, {len(feature_cols)} å€‹ç‰¹å¾µ")
        return X, y, feature_cols
    
    def walk_forward_train(self, df: pd.DataFrame, n_splits: int = 5):
        """
        æ™‚åºæ»¾å‹•é©—è­‰
        ä¾æ“šç”¨æˆ¶éœ€æ±‚ï¼šè¨“ç·´çª—å£ 24-36 å€‹æœˆ (ç°¡åŒ–ç‰ˆï¼šä½¿ç”¨ TimeSeriesSplit è‡ªå‹•åˆ‡åˆ†)
        """
        print(f"â³ é–‹å§‹ Walk-forward Validation (n_splits={n_splits})...")
        
        df = df.sort_values('date')
        dates = df['date'].unique()
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        metrics = []
        
        # æº–å‚™è³‡æ–™
        X_all, y_all, feature_cols = self.prepare_train_data(df)
        
        for i, (train_idx, val_idx) in enumerate(tscv.split(dates)):
            train_dates = dates[train_idx]
            val_dates = dates[val_idx]
            
            d_train = df[df['date'].isin(train_dates)]
            d_val = df[df['date'].isin(val_dates)]
            
            # ä½¿ç”¨ prepare_train_data ç¢ºä¿æ¬„ä½ä¸€è‡´
            X_train, y_train, _ = self.prepare_train_data(d_train)
            X_val, y_val, _ = self.prepare_train_data(d_val)
            
            params = self.best_params if self.best_params else self._get_default_params()
            
            lgb_train = lgb.Dataset(X_train, label=y_train)
            lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)
            
            # è¨“ç·´åˆ†é¡æ¨¡å‹
            model = lgb.train(
                params,
                lgb_train,
                num_boost_round=1000,
                valid_sets=[lgb_train, lgb_val],
                valid_names=['train', 'valid'],
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            # é æ¸¬æ©Ÿç‡
            preds_prob = model.predict(X_val)
            
            # è©•ä¼°æŒ‡æ¨™: AUC & LogLoss
            try:
                auc = roc_auc_score(y_val, preds_prob)
                loss = log_loss(y_val, preds_prob)
            except ValueError:
                auc = 0
                loss = 999
            
            metrics.append({'auc': auc, 'logloss': loss})
            print(f"  Fold {i+1} ({val_dates[0]}~{val_dates[-1]}): AUC={auc:.4f}, LogLoss={loss:.4f}")
            
        avg_auc = np.mean([m['auc'] for m in metrics])
        print(f"âœ… é©—è­‰å®Œæˆ. å¹³å‡ AUC: {avg_auc:.4f}")
        
        # æœ€çµ‚å…¨é‡è¨“ç·´ (å«æ ¡æº–æ‹†åˆ†)
        self.train_final_model(X_all, y_all, feature_cols)
        return metrics

    def optimize_params(self, X: pd.DataFrame, y: pd.Series, n_trials: int = 20):
        """Optuna è¶…åƒæ•¸èª¿å„ª"""
        print(f"â³ é–‹å§‹ Optuna è¶…åƒæ•¸èª¿å„ª (trials={n_trials})...")
        
        # å•Ÿå‹• MLflow å¯¦é©—
        mlflow.set_experiment("Agent_B_Stock_Prediction")
        
        def objective(trial):
            with mlflow.start_run(nested=True, run_name=f"Trial_{trial.number}"):
                param = {
                    'objective': 'binary',
                    'metric': 'auc',
                    'verbosity': -1,
                    'boosting_type': 'gbdt',
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                    'num_leaves': trial.suggest_int('num_leaves', 20, 150),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
                    'is_unbalance': True 
                }
                
                # ç´€éŒ„åƒæ•¸åˆ° MLflow
                mlflow.log_params(param)
                
                train_size = int(len(X) * 0.8)
                X_t, y_t = X.iloc[:train_size], y.iloc[:train_size]
                X_v, y_v = X.iloc[train_size:], y.iloc[train_size:]
                
                dtrain = lgb.Dataset(X_t, label=y_t)
                dval = lgb.Dataset(X_v, label=y_v, reference=dtrain)
                
                model = lgb.train(param, dtrain, num_boost_round=500, 
                                  valid_sets=[dval], callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)])
                
                preds = model.predict(X_v)
                try:
                    score = roc_auc_score(y_v, preds)
                except:
                    score = 0
                
                # ç´€éŒ„çµæœæŒ‡æ¨™åˆ° MLflow
                mlflow.log_metric("auc", score)
                
                return score # Maximize AUC

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        self.best_params = study.best_params
        self.best_params.update({'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'is_unbalance': True})
        
        print(f"âœ… æœ€ä½³åƒæ•¸: {self.best_params}")
        return self.best_params

    def train_final_model(self, X: pd.DataFrame, y: pd.Series, feature_names: list):
        """è¨“ç·´æœ€çµ‚æ¨¡å‹ + æ©Ÿç‡æ ¡æº– (Probability Calibration)"""
        params = self.best_params if self.best_params else self._get_default_params()
        
        # æ‹†åˆ† 10% åšæ ¡æº– (ä¾æ™‚é–“åºåˆ—ï¼Œå–æœ€å¾Œ 10%)
        # å› ç‚ºé€™æ˜¯ Time Seriesï¼Œä¸èƒ½éš¨æ©Ÿæ‹†
        calib_size = int(len(X) * 0.1)
        train_size = len(X) - calib_size
        
        X_train = X.iloc[:train_size]
        y_train = y.iloc[:train_size]
        X_calib = X.iloc[train_size:]
        y_calib = y.iloc[train_size:]
        
        print(f"â³ è¨“ç·´æœ€çµ‚æ¨¡å‹ (Train: {len(X_train)}, Calibration: {len(X_calib)})...")
        
        # å•Ÿå‹• MLflow çˆ¶é‹è¡Œ
        with mlflow.start_run(run_name="Final_Model_Training"):
            mlflow.log_params(params)
            mlflow.lightgbm.autolog()
            
            lgb_train = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)
            self.model = lgb.train(params, lgb_train, num_boost_round=500)
            
            # é€²è¡Œæ©Ÿç‡æ ¡æº– (Isotonic Regression)
            print("ğŸ”§ åŸ·è¡Œ Isotonic Probability Calibration...")
            raw_probs = self.model.predict(X_calib)
            self.calibrator = IsotonicRegression(out_of_bounds='clip')
            self.calibrator.fit(raw_probs, y_calib)
            
            # è¨˜éŒ„æ¨¡å‹åˆ° MLflow
            mlflow.lightgbm.log_model(self.model, "model")
            print(f"âœ… æœ€çµ‚æ¨¡å‹å·²ç´€éŒ„è‡³ MLflow")
            
        return self.model

    def _get_default_params(self):
        return {
            'objective': 'binary',
            'metric': 'auc',
            'is_unbalance': True,
            'verbose': -1
        }

    def save_model(self, filename: str = "latest_lgbm.pkl"):
        """å„²å­˜æ¨¡å‹èˆ‡æ ¡æº–å™¨"""
        if self.model is None: raise ValueError("å°šæœªè¨“ç·´æ¨¡å‹")
        model_path = self.model_dir / filename
        
        # å„²å­˜å­—å…¸åŒ…å«æ¨¡å‹èˆ‡æ ¡æº–å™¨
        save_obj = {
            'model': self.model,
            'calibrator': self.calibrator,
            'feature_names': self.model.feature_name()
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(save_obj, f)
        print(f"âœ“ æ¨¡å‹èˆ‡æ ¡æº–å™¨å·²å„²å­˜è‡³: {model_path}")

    def plot_feature_importance(self, top_n: int = 30):
        """ç¹ªè£½ç‰¹å¾µé‡è¦æ€§ (Gain)"""
        importance = self.model.feature_importance(importance_type='gain')
        features = self.model.feature_name()
        fi_df = pd.DataFrame({'feature': features, 'importance': importance}).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(12, 10))
        sns.barplot(data=fi_df.head(top_n), x='importance', y='feature', palette='magma')
        plt.title(f'Top {top_n} æ¨¡å‹ç‰¹å¾µé‡è¦æ€§ (Gain)')
        plt.tight_layout()
        plt.savefig(self.artifact_dir / "feature_importance.png", dpi=150)
        plt.close()
        print(f"âœ“ å·²ç”¢å‡ºç‰¹å¾µé‡è¦æ€§åœ–è¡¨")
        return fi_df
        
    def plot_shap_summary(self, X_sample: pd.DataFrame = None, sample_size: int = 1000):
        """ç¹ªè£½ SHAP Summary Plot (Scikit-Learn Skill Integration)"""
        if self.model is None:
            print("âš  æ¨¡å‹å°šæœªè¨“ç·´ï¼Œç„¡æ³•ç¹ªè£½ SHAP")
            return

        print("â³ è¨ˆç®— SHAP values...")
        try:
            # ä½¿ç”¨ TreeExplainer (é‡å° LightGBM å„ªåŒ–)
            explainer = shap.TreeExplainer(self.model)
            
            if X_sample is None:
                # è‹¥æœªæä¾›ï¼Œå˜—è©¦è‡ªå‹•è¼‰å…¥ä¸€éƒ¨åˆ†è³‡æ–™
                try:
                    df = self.load_features()
                    X, _, _ = self.prepare_train_data(df)
                    # éš¨æ©Ÿæ¡æ¨£ä»¥åŠ é€Ÿ
                    if len(X) > sample_size:
                        X_sample = X.sample(n=sample_size, random_state=42)
                    else:
                        X_sample = X
                except Exception as e:
                    print(f"âš  ç„¡æ³•è‡ªå‹•è¼‰å…¥è³‡æ–™ä¾› SHAP åˆ†æ: {e}")
                    return

            shap_values = explainer.shap_values(X_sample)
            
            # LightGBM binary classification returns list of arrays [class0, class1] or just class1 depending on version
            # New LightGBM versions with SHAP might return array. Handle carefully.
            if isinstance(shap_values, list):
                # Binary classification: index 1 is positive class
                shap_vals_target = shap_values[1]
            else:
                shap_vals_target = shap_values

            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_vals_target, X_sample, show=False)
            plt.title("SHAP Summary Plot (Top Features)")
            plt.tight_layout()
            
            save_path = self.artifact_dir / "shap_summary.png"
            plt.savefig(save_path, dpi=150)
            plt.close()
            print(f"âœ“ å·²ç”¢å‡º SHAP Summary Plot: {save_path}")
            
        except Exception as e:
            print(f"âŒ SHAP åˆ†æå¤±æ•—: {e}")


def main():
    print("ğŸš€ Agent B æ¨¡å‹å„ªåŒ–è¨“ç·´å•Ÿå‹• (Miniç‰ˆ - Classification)...")
    trainer = LightGBMTrainer()
    
    try:
        # 1. æº–å‚™è³‡æ–™
        df = trainer.load_features()
        # ä½¿ç”¨ LabelGenerator ç”Ÿæˆ D+1 æ¨™ç±¤
        df = trainer.generate_labels(df)
        
        # 2. è‡ªå‹•èª¿å„ª
        X, y, feature_cols = trainer.prepare_train_data(df)
        trainer.optimize_params(X, y, n_trials=100) # Weekend Mode: 100 trials
        
        # 3. æ™‚åºæ»¾å‹•é©—è­‰èˆ‡æœ€çµ‚è¨“ç·´
        trainer.walk_forward_train(df)
        
        # 4. å„²å­˜èˆ‡ç”¢å‡ºåˆ†æ
        trainer.save_model()
        trainer.plot_feature_importance()
        
        print("\nâœ¨ æ¨¡å‹å„ªåŒ–æµç¨‹å·²åœ“æ»¿å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµç¨‹ä¸­æ–·: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()
